## Задания по распределленным системам
Четыре лабораторки с такими баллами и сроками сдачи:
- 1, 15 баллов, до 31 Марта, Hadoop
- 2, 20 баллов, до 29 Апреля, Map-Reduce
- 3, 25 баллов, до 31 мая, Apache Spark

Задание можно сдать после срока, но не позже чем за неделю до экзамена, за 2/3 стоимости. Задание можно пересдать за 3/4 стоимости. Оптимальная форма сдачи задания - прислать по почте (просьба  тексте письма указывать чья работа).

Лабораторную можно делать вдвоем, при этом в письме следует указать обоих авторов. Первые две лабораторные можно сдать просто удаленно. Для третьей и четвертой помимо присланного задания надо будет также пообщаться личнно.

Если сданные работы будут черезмерно похожи, это будет считаться читерством и засчитываться такие работы не будут. Читерские работы можно один раз пересдать за 1/2 стоимости. Заметьте, что такого результата можно достигнуть не только переписывая друг у друга, но и независимо переписывая из одних и тех же открытых источников.

[Ссылка на результаты](https://docs.google.com/spreadsheets/d/1s3RHyYcPYYN42jWtBuHBqvAE979w8cgKLZ0UvbEapdU/edit?usp=sharing)

#### Пары
График встреч, как мы с вами договаривались, раз в две недели с каждой группой, чередуя четверги и пятницы. Пятому курсу если надо встретиться, просьба приходить на пару к четвертому курсу.

- 26.02, Пятница, 3я пара
- 03.03, Четверг, 2я пара
- 11.03, Пятница, 3я пара
- 17.03, Четверг, 2я пара
- 25.03, Пятница, 3я пара
- 31.03, Четверг, 2я пара


### Лаба 1 - Apache Spark, подсчет треугольников, до 27 марта, xxx баллов

По данному файлу содержащему список ребер (неориентированного) графа написать Spark-приложение, считающее в этом графе количество треугольников.

Файл состоит из строк, каждая содержит два числа - вершины соответствующего ребра. Файл `graph.txt` прилагается.

Для этого вам надо, например, создать RDD из файла, потом преобраззовать его в список пар при помощи map, затем несколькими операциями join выделить в нем треугольники и возвратить результат при помощи действия count. Для ускорения работы промежуточное RDD со списком ребер можно закешировать операцией cashe.

Скачать Spark можно [здесь](http://spark.apache.org/downloads.html), рекомендуется скачивать собранную версию со встроенным хадупом, т.к. ее можно с ходу запускать. Инструкции по написанию и запуску приложений на Spark [здесь](http://spark.apache.org/docs/latest/programming-guide.html). С моей точки зрения, легче всего реализовывать на питоне т.к. можно не компилировать, а просто отправлять .py файл на выполнение. Для java надо сначало скомпилировать в jar, а потом уже запускать jar. Файл с графом Spark может брать прямо из (запущенного) Hadoop-кластера, для это файл надо указывать по его сетевому имени в Hadoop, например так

```
val file = sc.textFile("hdfs://localhost:9000/mr_test/input/anonymous-msweb.data").
```

### Лаба 2 - Apache Spark, инвертирование индекса, до 13 апреля, xxx баллов
Построить [инвертированный индекс](https://ru.wikipedia.org/wiki/Инвертированный_индекс) для датасета представляющего собой набор сообщений. Данные представляют собой файлы с текстом сообщений разложенные по каталогам. Имя файла - это индекс записи.  

В результате должен получиться файл `index.csv` содержащий три поля для каждого слова: слово, общее количество вхождений этого слова (по всем документам), список документов через пробел. Например:

```
  accept,258,54243 60890 60884 76067 105252
  crud, 4, 104959 16152
```
(результат приведен только для примера)

При считывании файлов следует:
  - исключить строки со служебной информацией: `Path:`, `Newsgroups:`, `xxx writes:`, ...;
  - запятые, точки, пробелы, etc. считать разделителями;
  - брать только слова, состоящие из латинсих букв и символа `'`;
  - все слова привести к нижнему регистру;
  - вообще, ориентирутесь на здравый смысл, посмотрите глазами на список слов, который получается, чтоб в него не попал мусор, н с другой стороны чтоб он отражал структуру текста, за исключением, возможно, небольшого количества плохих случаев;


Данные взяты [отсюда] (http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)

### Лаба 3 - Spark SQL, до 20 апреля, xxx баллов




