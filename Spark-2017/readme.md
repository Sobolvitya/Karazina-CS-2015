## Задания по распределленным системам
Четыре или пять лабораторок с сроками сдачи (баллы будут чуть позже):
- 1,  до 27 Марта, Apache Spark, подсчёт треугольников;
- 2,  до 13 Апреля, Apache Spark, инвертированный индекс;
- 3,  до 20 Апреля, Spark SQL;
- 4, Communicating Sequential Processes (CSP): gopher (Scala) / CHP (Haskell);
- 5, что-то еще про Parallel Functional Programming (или же ничего).

Сдача после срока иногда возможна но не приветствуется (со штрафом или вообще нет). Оптимальная форма сдачи задания - прислать по почте (просьба  тексте письма указывать чья работа). Лабораторную можно делать вдвоем, при этом в письме следует указать обоих авторов. Если у меня будут возникать вопросы по работе, я могу попросить автора(авторов) встретиться для сдачи лично.

Если сданные работы будут черезмерно похожи, это будет считаться читерством и засчитываться такие работы не будут. Заметьте, что такого результата можно достигнуть не только переписывая друг у друга, но и независимо переписывая из одних и тех же открытых источников.

### Лаба 1 - Apache Spark, подсчет треугольников, до 27 марта, 10 баллов

По данному файлу содержащему список ребер (неориентированного) графа написать Spark-приложение, считающее в этом графе количество треугольников.

Файл состоит из строк, каждая содержит два числа - вершины соответствующего ребра. Файл `graph.txt` прилагается.

Для этого вам надо, например, создать RDD из файла, потом преобраззовать его в список пар при помощи map, затем несколькими операциями join выделить в нем треугольники и возвратить результат при помощи действия count. Для ускорения работы промежуточное RDD со списком ребер можно закешировать операцией cashe.

Скачать Spark можно [здесь](http://spark.apache.org/downloads.html), рекомендуется скачивать собранную версию со встроенным хадупом, т.к. ее можно с ходу запускать. Инструкции по написанию и запуску приложений на Spark [здесь](http://spark.apache.org/docs/latest/programming-guide.html). С моей точки зрения, легче всего реализовывать на питоне т.к. можно не компилировать, а просто отправлять .py файл на выполнение. Для java надо сначало скомпилировать в jar, а потом уже запускать jar. Файл с графом Spark может брать прямо из (запущенного) Hadoop-кластера, для это файл надо указывать по его сетевому имени в Hadoop, например так

```
val file = sc.textFile("hdfs://localhost:9000/mr_test/input/anonymous-msweb.data").
```

### Лаба 2 - Apache Spark, инвертирование индекса, до 13 апреля, 10 баллов
Построить [инвертированный индекс](https://ru.wikipedia.org/wiki/Инвертированный_индекс) для датасета представляющего собой набор сообщений. Данные представляют собой файлы с текстом сообщений разложенные по каталогам. Имя файла - это индекс записи.  

В результате должен получиться файл `index.csv` содержащий три поля для каждого слова: слово, общее количество вхождений этого слова (по всем документам), список документов через пробел. Например:

```
  accept,258,54243 60890 60884 76067 105252
  crud, 4, 104959 16152
```
(результат приведен только для примера)

При считывании файлов следует:
  - исключить строки со служебной информацией: `Path:`, `Newsgroups:`, `xxx writes:`, ...;
  - запятые, точки, пробелы, etc. считать разделителями;
  - брать только слова, состоящие из латинсих букв и символа `'`;
  - все слова привести к нижнему регистру;
  - вообще, ориентирутесь на здравый смысл, посмотрите глазами на список слов, который получается, чтоб в него не попал мусор, н с другой стороны чтоб он отражал структуру текста, за исключением, возможно, небольшого количества плохих случаев;


Данные взяты [отсюда] (http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)

### Лаба 3 - Twitter, Streaming & Spark SQL , до 4 мая, 15 баллов

Написать скрипт, который с каждые 20 секунд будет считать общее количество твитов сделанных в последние две минуты, содержащие определенный хэш-тег (какой - выберите сами), а также топ-10 слов (и их количество), которые попадаются в этих твитах. Сделайте, пожалуйста, основную обработку данных прри помощи SparkSQL - это просто, по сути вы пишете обычный SQL-запрос. В Rdd вам надо оставить только первичную обработку входного потока, вроде разбивания на слова. Временные рамки 2 минуты с 20 сек. обновлением нужны скорее для удобства тестирования. Для более содержательных результатов интервал можно увеличить.

В совокупности, вам надо будет применить [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html) - достаточно формально, и [Spark Streaming](http://spark.apache.org/docs/latest/streaming-programming-guide.html) - чуть подробнее.

Для того, чтоб подключиться к твиттеру, для начала надо завести там аккаунт и получить (Twitter API Credentials)[https://databricks-training.s3.amazonaws.com/realtime-processing-with-spark-streaming.html#twitter-credential-setup].

В Java и Scala стриминг из твиттера включен в Spark, [org.apache.spark.streaming.twitter](https://spark.apache.org/docs/1.6.2/streaming-programming-guide.html#advanced-sources) (правда эта библиотека вроде как не включена в стандартный пакет, ее надо доставлять отдельно, через Maven - у меня не получилось).

В питоне родная интеграция твиттера в SparkStreaming не реализована, но можно сделать или [при поомощи socketTextStream](http://www.awesomestats.in/spark-twitter-stream/)(обратите внимание, что тут запускаются два процесса), или при помощи [Kafka](https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/).







