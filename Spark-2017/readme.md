## Задания по распределленным системам
Четыре или пять лабораторок с сроками сдачи (баллы будут чуть позже):
- 1,  до 27 Марта, Apache Spark, подсчёт треугольников;
- 2,  до 13 Апреля, Apache Spark, инвертированный индекс;
- 3,  до 20 Апреля, Spark SQL;
- 4, Communicating Sequential Processes (CSP): gopher (Scala) / CHP (Haskell);
- 5, что-то еще про Parallel Functional Programming (или же ничего).

Сдача после срока иногда возможна но не приветствуется (со штрафом или вообще нет). Оптимальная форма сдачи задания - прислать по почте (просьба  тексте письма указывать чья работа). Лабораторную можно делать вдвоем, при этом в письме следует указать обоих авторов. Если у меня будут возникать вопросы по работе, я могу попросить автора(авторов) встретиться для сдачи лично.

Если сданные работы будут черезмерно похожи, это будет считаться читерством и засчитываться такие работы не будут. Заметьте, что такого результата можно достигнуть не только переписывая друг у друга, но и независимо переписывая из одних и тех же открытых источников.

### Лаба 1 - Apache Spark, подсчет треугольников, до 27 марта, xxx баллов

По данному файлу содержащему список ребер (неориентированного) графа написать Spark-приложение, считающее в этом графе количество треугольников.

Файл состоит из строк, каждая содержит два числа - вершины соответствующего ребра. Файл `graph.txt` прилагается.

Для этого вам надо, например, создать RDD из файла, потом преобраззовать его в список пар при помощи map, затем несколькими операциями join выделить в нем треугольники и возвратить результат при помощи действия count. Для ускорения работы промежуточное RDD со списком ребер можно закешировать операцией cashe.

Скачать Spark можно [здесь](http://spark.apache.org/downloads.html), рекомендуется скачивать собранную версию со встроенным хадупом, т.к. ее можно с ходу запускать. Инструкции по написанию и запуску приложений на Spark [здесь](http://spark.apache.org/docs/latest/programming-guide.html). С моей точки зрения, легче всего реализовывать на питоне т.к. можно не компилировать, а просто отправлять .py файл на выполнение. Для java надо сначало скомпилировать в jar, а потом уже запускать jar. Файл с графом Spark может брать прямо из (запущенного) Hadoop-кластера, для это файл надо указывать по его сетевому имени в Hadoop, например так

```
val file = sc.textFile("hdfs://localhost:9000/mr_test/input/anonymous-msweb.data").
```

### Лаба 2 - Apache Spark, инвертирование индекса, до 13 апреля, xxx баллов
Построить [инвертированный индекс](https://ru.wikipedia.org/wiki/Инвертированный_индекс) для датасета представляющего собой набор сообщений. Данные представляют собой файлы с текстом сообщений разложенные по каталогам. Имя файла - это индекс записи.  

В результате должен получиться файл `index.csv` содержащий три поля для каждого слова: слово, общее количество вхождений этого слова (по всем документам), список документов через пробел. Например:

```
  accept,258,54243 60890 60884 76067 105252
  crud, 4, 104959 16152
```
(результат приведен только для примера)

При считывании файлов следует:
  - исключить строки со служебной информацией: `Path:`, `Newsgroups:`, `xxx writes:`, ...;
  - запятые, точки, пробелы, etc. считать разделителями;
  - брать только слова, состоящие из латинсих букв и символа `'`;
  - все слова привести к нижнему регистру;
  - вообще, ориентирутесь на здравый смысл, посмотрите глазами на список слов, который получается, чтоб в него не попал мусор, н с другой стороны чтоб он отражал структуру текста, за исключением, возможно, небольшого количества плохих случаев;


Данные взяты [отсюда] (http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)

### Лаба 3 - Spark SQL, до 20 апреля, xxx баллов




